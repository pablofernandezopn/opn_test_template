# Question Chat Edge Function

Sistema de chat contextual para preguntas de examen con integraciÃ³n opcional de RAG (Retrieval-Augmented Generation).

## Arquitectura

### Modo por Defecto: CompletaciÃ³n Directa con OpenAI
Por defecto, la edge function usa **completaciones directas de OpenAI** basadas en el contexto de la pregunta, sin acceder a la base de datos legal (RAG).

**Ventajas:**
- âœ… Respuestas rÃ¡pidas (1-2 segundos)
- âœ… Conversaciones naturales y contextuales
- âœ… Menor coste operativo
- âœ… Perfecto para explicar opciones, ayudar con la comprensiÃ³n, etc.

### Modo RAG: BÃºsqueda Legal (Opcional)
El usuario puede **forzar el uso de RAG** cuando necesita informaciÃ³n legal especÃ­fica mediante el parÃ¡metro `force_rag: true`.

**Ventajas:**
- âœ… Acceso a base de datos legal completa
- âœ… Referencias legales precisas con citas
- âœ… InformaciÃ³n actualizada de leyes y cÃ³digos
- âš ï¸ Mayor tiempo de respuesta (10-60 segundos)
- âš ï¸ Mayor coste por consulta

## API Request

```typescript
interface QuestionChatRequest {
  question_id: number
  message?: string
  user_answer?: number
  user_test_id?: number
  include_user_stats?: boolean
  extra_context?: string
  force_rag?: boolean  // ğŸ”‘ NUEVO: Fuerza uso de RAG API
}
```

### Ejemplos de Uso

#### 1. Consulta Normal (Sin RAG - Por Defecto)
```bash
curl -X POST 'http://localhost:54321/functions/v1/question-chat' \
  -H 'Authorization: Bearer YOUR_JWT_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "question_id": 1,
    "message": "No entiendo por quÃ© la opciÃ³n 2 es correcta"
  }'
```

**Respuesta:** ExplicaciÃ³n basada en el tip/contexto de la pregunta usando OpenAI directamente.

#### 2. Consulta con BÃºsqueda Legal (Con RAG)
```bash
curl -X POST 'http://localhost:54321/functions/v1/question-chat' \
  -H 'Authorization: Bearer YOUR_JWT_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "question_id": 1,
    "message": "Â¿QuÃ© dice el artÃ­culo 17 de la ConstituciÃ³n sobre esto?",
    "force_rag": true
  }'
```

**Respuesta:** BÃºsqueda en base de datos legal con citas y referencias precisas.

## Flujo de DecisiÃ³n

```mermaid
graph TD
    A[Usuario envÃ­a mensaje] --> B{force_rag = true?}
    B -->|SÃ­| C[Usar RAG API]
    B -->|No| D[CompletaciÃ³n directa OpenAI]
    C --> E[Respuesta con citas legales]
    D --> F[Respuesta contextual rÃ¡pida]
```

## Logs y Debugging

### Log de DecisiÃ³n de Routing
```
ğŸ¯ Routing decision: Direct OpenAI completion
ğŸ’­ Reasoning: Modo por defecto: completaciÃ³n directa con OpenAI (sin RAG)
```

O cuando se fuerza RAG:
```
ğŸ¯ Routing decision: Use RAG API
ğŸ’­ Reasoning: Usuario solicitÃ³ explÃ­citamente uso de RAG (force_rag=true)
```

### Metadata en Mensajes
Cada mensaje guardado incluye en su metadata:
- `routing_decision`: RazÃ³n de la decisiÃ³n de routing
- `force_rag`: Si el usuario forzÃ³ el uso de RAG
- `source`: Origen de la respuesta (`openai_direct` o `rag_api`)

## ConfiguraciÃ³n

### Variables de Entorno
```env
OPEN_AI_KEY=sk-proj-...
RAG_API_URL=https://rag-legal-api-...
```

### OpenAI Model
- Completaciones directas: `gpt-5-mini` (rÃ¡pido y econÃ³mico)
- Temperature: `0.7` (conversacional pero consistente)
- Max tokens: `800` (respuestas concisas)

## Testing

### Test 1: Modo Normal (Sin RAG)
```bash
{
  "question_id": 1,
  "message": "Hola, Â¿puedes explicarme esta pregunta?"
}
```
Respuesta esperada: ExplicaciÃ³n amigable usando el contexto de la pregunta.

### Test 2: Modo RAG
```bash
{
  "question_id": 1,
  "message": "Â¿QuÃ© dice el CÃ³digo Penal sobre este tema?",
  "force_rag": true
}
```
Respuesta esperada: BÃºsqueda legal con citas y referencias.

## Performance

| Modo | Tiempo Respuesta | Coste Estimado | Uso Recomendado |
|------|-----------------|----------------|-----------------|
| Direct OpenAI | 1-2s | Bajo | Explicaciones, ayuda general |
| RAG API | 10-60s | Alto | Consultas legales especÃ­ficas |

## Mejoras Futuras

- [ ] CachÃ© de respuestas frecuentes
- [ ] DetecciÃ³n automÃ¡tica de necesidad de RAG (opcional)
- [ ] Streaming de respuestas para mejor UX
- [ ] Historial conversacional con contexto completo
